# Lee LLM Router â€” example configuration
# Run `lee-llm-router template` to print this file.
# Copy to config/llm.yaml and fill in your values.
#
# All api_key_env values are *environment variable names*, not secrets.
# The router reads os.environ[api_key_env] at call time.

llm:
  # Role used when the caller does not specify one.
  default_role: planner

  providers:
    openrouter:
      type: openrouter_http
      base_url: https://openrouter.ai/api/v1
      # Set OPENROUTER_API_KEY in your environment
      api_key_env: OPENROUTER_API_KEY

    codex_local:
      type: codex_cli
      command: codex
      model_flag: --model
      output_flag: --output-last-message

    mock:
      type: mock
      # Optional: override the default echo text
      # response_text: "static response for testing"

  roles:
    planner:
      provider: openrouter
      model: openai/gpt-4o
      temperature: 0.2
      json_mode: false
      timeout: 60.0
      # On failure, try codex_local before giving up (Phase 2)
      fallback_providers: [codex_local]

    extractor:
      provider: openrouter
      model: openai/gpt-4o-mini
      temperature: 0.0
      json_mode: true
      max_tokens: 2048

    local:
      provider: codex_local
      model: o3

    test:
      provider: mock
